{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3ed4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are running this code in Google Colab, please make sure to run the following code first\n",
    "!pip install numpy\n",
    "!pip install --upgrade  cython\n",
    "!pip install --upgrade scipy\n",
    "!pip install mlflow\n",
    "\n",
    "%cd /usr/local/lib/python3.10/dist-packages\n",
    "!git clone https://github.com/microsoft/qlib.git\n",
    "%cd qlib\n",
    "!pip install --upgrade jupyter-client\n",
    "!pip install .\n",
    "!python setup.py install\n",
    "%cd ~\n",
    "!pip install statsmodels plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78a669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import qlib\n",
    "from qlib.workflow.record_temp import SignalRecord, PortAnaRecord, SigAnaRecord\n",
    "from qlib.workflow import R\n",
    "from qlib.utils import init_instance_by_config, flatten_dict\n",
    "from qlib.contrib.report import analysis_model, analysis_position\n",
    "from qlib.data.dataset.handler import DataHandlerLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f4cc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define market and benchmark\n",
    "market = \"tw50\"  # Taiwan 50 index constituents\n",
    "benchmark = \"0050\"  # Taiwan 50 ETF as benchmark\n",
    "\n",
    "# Initialize QLib with Taiwan market data\n",
    "qlib.init(provider_uri=\"data/tw_data\",  # Set your data path here\n",
    "          region=\"tw\",\n",
    "          dataset_cache=\"SimpleDatasetCache\")  # Cache setting for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04fe793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null counts in each column:\n",
      "$open      0\n",
      "$high      0\n",
      "$low       0\n",
      "$close     0\n",
      "$volume    0\n",
      "dtype: int64\n",
      "zero counts in each column:\n",
      "$open        0\n",
      "$high        0\n",
      "$low         0\n",
      "$close       0\n",
      "$volume    327\n",
      "dtype: int64\n",
      "negative counts in each column:\n",
      "$open      0\n",
      "$high      0\n",
      "$low       0\n",
      "$close     0\n",
      "$volume    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from qlib.data import D\n",
    "\n",
    "# Fetch all available instruments for Taiwan market\n",
    "instruments = D.instruments(\"all\")\n",
    "\n",
    "# Define fields to fetch (OHLCV data)\n",
    "fields = [\"$open\", \"$high\", \"$low\", \"$close\", \"$volume\"]\n",
    "\n",
    "# Define the time range (10-year period)\n",
    "start_time = \"2014-12-31\"\n",
    "end_time = \"2024-12-30\"\n",
    "\n",
    "# Fetch the historical price data\n",
    "df = D.features(instruments, fields, start_time=start_time, end_time=end_time)\n",
    "\n",
    "# Check data quality - null values\n",
    "print(f\"null counts in each column:\\n{df.isnull().sum() }\")\n",
    "\n",
    "# Check data quality - zero values\n",
    "zero_counts = (df == 0).sum()\n",
    "print(f\"zero counts in each column:\\n{zero_counts}\")\n",
    "\n",
    "# Check data quality - negative values\n",
    "negative_counts = (df < 0).sum()\n",
    "print(f\"negative counts in each column:\\n{negative_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882946b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_label = [\"Ref($close, -1)/$close - 1\"]  # next-day returns\n",
    "\n",
    "# Define custom processors for data handling\n",
    "# Data processors for training and inference\n",
    "custom_learn_processors = [\n",
    "    {\"class\": \"CSZScoreNorm\", \"kwargs\": {\"fields_group\": \"label\"}},\n",
    "    {\"class\": \"DropnaLabel\"},\n",
    "]\n",
    "\n",
    "custom_infer_processors = [\n",
    "    {\"class\": \"ZScoreNorm\", \"kwargs\": {}},\n",
    "    {\"class\": \"Fillna\", \"kwargs\": {}},\n",
    "]\n",
    "\n",
    "data_handler_config = {\n",
    "    \"start_time\": \"2014-12-31\",\n",
    "    \"end_time\": \"2024-12-27\",\n",
    "    \"fit_start_time\": \"2014-12-31\",\n",
    "    \"fit_end_time\": \"2021-12-31\",\n",
    "    \"instruments\": market,\n",
    "    \"label\": custom_label,\n",
    "    \"learn_processors\": custom_learn_processors,\n",
    "    \"infer_processors\": custom_infer_processors,\n",
    "}\n",
    "\n",
    "# Define model training configuration\n",
    "task = {\n",
    "    # Model configuration\n",
    "    \"model\": {\n",
    "        \"class\": \"XGBModel\",\n",
    "        \"module_path\": \"qlib.contrib.model.xgboost\",\n",
    "        \"kwargs\": {\n",
    "            \"eval_metric\": \"rmse\",\n",
    "            \"learning_rate\": 0.06424642669823001,\n",
    "            \"max_depth\": 5,\n",
    "            \"subsample\": 0.8466671252590435,\n",
    "            \"colsample_bytree\": 0.962838237653785,\n",
    "            \"reg_alpha\": 0.49091134749293946,\n",
    "            \"reg_lambda\": 0.4299807160698682\n",
    "        }\n",
    "    },\n",
    "    # Dataset configuration\n",
    "    \"dataset\": {\n",
    "        \"class\": \"DatasetH\",\n",
    "        \"module_path\": \"qlib.data.dataset\",\n",
    "        \"kwargs\": {\n",
    "            \"handler\": {\n",
    "                \"class\": \"Alpha158\",\n",
    "                \"module_path\": \"qlib.contrib.data.handler\",\n",
    "                \"kwargs\": data_handler_config,\n",
    "            },\n",
    "            \"segments\": {\n",
    "                \"train\": (\"2014-12-31\", \"2021-12-31\"),\n",
    "                \"valid\": (\"2022-01-01\", \"2022-12-31\"),\n",
    "                \"test\": (\"2023-01-01\", \"2024-12-27\"),\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize model and dataset from configuration\n",
    "model = init_instance_by_config(task[\"model\"])\n",
    "dataset = init_instance_by_config(task[\"dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f981934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Examine the dataset before and after preprocessing\n",
    "def examine_data_segment(data_dict, segment_name, data_type_name):\n",
    "    print(f\"--- {data_type_name} {segment_name.capitalize()} ---\")\n",
    "    if data_dict is None or not isinstance(data_dict, (pd.DataFrame, pd.Series)):\n",
    "        print(\"Data not available or not a DataFrame/Series.\")\n",
    "        return\n",
    "\n",
    "    nan_count = data_dict.isna().sum().sum()\n",
    "    print(f\"NaNs: {nan_count}\")\n",
    "\n",
    "    inf_count = np.isinf(data_dict.values).sum()\n",
    "    print(f\"Infs: {inf_count}\")\n",
    "\n",
    "    print(f\"Shape: {data_dict.shape}\")\n",
    "\n",
    "\n",
    "# --- 1. Raw Data Examination ---\n",
    "print(\"=\"*10 + \" Raw Data Examination \" + \"=\"*10)\n",
    "segments = [\"train\", \"valid\", \"test\"]\n",
    "col_set = [\"feature\", \"label\"]\n",
    "\n",
    "for segment in segments:\n",
    "    try:\n",
    "        raw_data = dataset.prepare(\n",
    "            segment,\n",
    "            col_set=col_set,\n",
    "            data_key=DataHandlerLP.DK_R\n",
    "        )\n",
    "        examine_data_segment(raw_data.get('feature'), segment, \"Raw Feature\")\n",
    "        examine_data_segment(raw_data.get('label'), segment, \"Raw Label\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading raw data for segment '{segment}': {e}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- 2. Preprocessed Data Examination ---\n",
    "print(\"\\n\" + \"=\"*10 + \" Preprocessed Data Examination \" + \"=\"*10)\n",
    "processed_data = {}\n",
    "\n",
    "for segment in [\"train\", \"valid\"]:\n",
    "    try:\n",
    "        processed_data[segment] = dataset.prepare(\n",
    "            segment,\n",
    "            col_set=col_set,\n",
    "            data_key=DataHandlerLP.DK_L\n",
    "        )\n",
    "        examine_data_segment(processed_data[segment].get(\n",
    "            'feature'), segment, \"Processed Feature\")\n",
    "        examine_data_segment(processed_data[segment].get(\n",
    "            'label'), segment, \"Processed Label\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading preprocessed data for segment '{segment}': {e}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "segment = \"test\"\n",
    "try:\n",
    "    processed_data[segment] = {'feature': None, 'label': None}\n",
    "    processed_data[segment]['feature'] = dataset.prepare(\n",
    "        segment,\n",
    "        col_set=[\"feature\"],\n",
    "        data_key=DataHandlerLP.DK_I\n",
    "    )\n",
    "    examine_data_segment(\n",
    "        processed_data[segment]['feature'], segment, \"Processed Feature (DK_I)\")\n",
    "\n",
    "    processed_data[segment]['label'] = dataset.prepare(\n",
    "        segment,\n",
    "        col_set=[\"label\"],\n",
    "        data_key=DataHandlerLP.DK_L\n",
    "    )\n",
    "    examine_data_segment(\n",
    "        processed_data[segment]['label'], segment, \"Processed Label (DK_L)\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading preprocessed data for segment '{segment}': {e}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2260eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment name\n",
    "experiment_type = \"TRAIN\"\n",
    "feature_set = \"Alpha158\"\n",
    "\n",
    "# Start QLib recorder for experiment tracking\n",
    "with R.start(experiment_name=f\"{experiment_type}_{market}_{task['model']['class']}_{feature_set}\"):\n",
    "    # Log model parameters for traceability\n",
    "    print(\"Logging model parameters\")\n",
    "    R.log_params(**flatten_dict(task))\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training model\")\n",
    "    model.fit(dataset)\n",
    "\n",
    "    # Save trained model\n",
    "    R.save_objects(**{\"params.pkl\": model})\n",
    "    rid = R.get_recorder().id\n",
    "\n",
    "    # Generate predictions for test set\n",
    "    print(\"Generating predictions\")\n",
    "    recorder = R.get_recorder()\n",
    "    sr = SignalRecord(model, dataset, recorder)\n",
    "    sr.generate()\n",
    "\n",
    "    # Signal Analysis\n",
    "    print(\"Performing signal analysis\")\n",
    "    sar = SigAnaRecord(recorder)  # IC, ICIR, Rank IC and Rank ICIR\n",
    "    sar.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff8f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prediction and label data\n",
    "pred_df = recorder.load_object(\"pred.pkl\")\n",
    "label_df = recorder.load_object(\"label.pkl\")\n",
    "label_df.columns = [\"label\"]\n",
    "\n",
    "# Get normalized label data\n",
    "label_df_normalized = dataset.prepare(\n",
    "    \"test\", col_set=[\"label\"], data_key=DataHandlerLP.DK_I)\n",
    "label_df_normalized.columns = [\"label\"]\n",
    "\n",
    "# Create combined dataframes for raw and normalized data\n",
    "pred_label = pd.concat([label_df, pred_df],\n",
    "                       axis=1, sort=True).reindex(label_df.index)\n",
    "pred_label_normalized = pd.concat(\n",
    "    [label_df_normalized, pred_df], axis=1, sort=True).reindex(label_df.index)\n",
    "\n",
    "# Clean up index levels if needed\n",
    "for df in [pred_label, pred_label_normalized]:\n",
    "    if df.index.nlevels > 2:\n",
    "        df.drop(level=0, inplace=True)\n",
    "\n",
    "# Evaluate\n",
    "print(\"===Calculating evaluation loss for raw label===\")\n",
    "print(f\"\\nraw label: {pred_label['label'].head()}\")\n",
    "print(f\"\\npred: {pred_label['score'].head()}\")\n",
    "mse = mean_squared_error(pred_label['label'], pred_label['score'])\n",
    "print(f\"\\nMean Squared Error: {mse:.6f}\")\n",
    "\n",
    "print(\"\\n===Calculating evaluation loss for normalized label===\")\n",
    "print(f\"\\nnormalized label: {pred_label_normalized['label'].head()}\")\n",
    "print(f\"\\npred: {pred_label_normalized['score'].head()}\")\n",
    "mse_normalized = mean_squared_error(\n",
    "    pred_label_normalized['label'],\n",
    "    pred_label_normalized['score']\n",
    ")\n",
    "print(f\"\\nMean Squared Error (Normalized): {mse_normalized:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389021a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance\n",
    "analysis_model.model_performance_graph(pred_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
